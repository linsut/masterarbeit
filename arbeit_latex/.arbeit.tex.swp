%% Dokumenteinstellungen
\documentclass[12pt,german]{report}

%% Verwendete Pakete
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage{geometry}
\geometry{verbose,a4paper,tmargin=4cm,bmargin=4cm,lmargin=2.8cm,rmargin=2.8cm}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{setspace}
\usepackage{listings}
\titleformat{\chapter}{\huge}{\thechapter.}{20pt}{\huge}

%% Einstellungen
\setlength{\oddsidemargin}{0cm}
\setlength{\topmargin}{-1.5cm}
\setlength{\textheight}{22cm}
\setlength{\textwidth}{16cm}
\setlength{\headsep}{1cm}
\setlength{\parindent}{0ex}
\setlength{\parskip}{2.0ex plus 0.9ex minus 0.4ex}

\begin{document}

%% Titelblatt
\input{ba_titel.tex}
\newpage 
\thispagestyle{empty}
\quad 
\newpage

%% Inhaltsverzeichnis
\tableofcontents
\newpage 
\thispagestyle{empty}
\quad 
\newpage

%% Formaljuristisches
\input{ba_jurist.tex}
\newpage 
\thispagestyle{empty}
\quad 
\newpage

%% Einzeldokumente, z.B. in Kapitel aufgeteilt

\begin{onehalfspace}
\input{kapitel/kapitel0.tex}
\input{kapitel/kapitel1.tex}
\input{kapitel/kapitel2.tex}
\input{kapitel/kapitel3.tex}
\end{onehalfspace}

%% Abbildungsverzeichnis
\listoffigures

\begin{thebibliography}{}
%% Zitieren Sie diesen ersten Eintrag mit \cite{bib_1}
\bibitem{ulm} Howard, J., Ruder, S., Universal Language Model Fine-tuning for Text Classification. 2018,  arXiv:1801.06146 
\bibitem{attention} Vaswani, A. et al., Attention Is All You Need. 2017,  arXiv:1706.03762v5 
\bibitem{elmo} Peters, M.,  Neumann, M.,  Iyyer, M., Gardner, M.,  Clark, C. , Lee, K., Zettlemoyer, L.,  Deep contextualized word representations. 2018, arXiv:1802.05365v2
\bibitem{elmoex} Mihail, E., Deep Contextualized Word Representations with ELMo. 2018, https://www.mihaileric.com/posts/deep-contextualized-word-representations-elmo/
\bibitem{bert} Devlin, J., Chang, M., Lee K. , Toutanova, K., BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding. 2019, arXiv:1810.04805v2
\bibitem{gpt} Radford,  A.,  Narasimhan,  K.,  Salimans,  T.,  and  Sutskever,  I., Improving language understanding by generative pre-training. 2018, https://openai.com/blog/language-unsupervised/
\bibitem{gpt2} Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., Language Models are Unsupervised Multitask Learners. 2018, https://openai.com/blog/better-language-models/
\bibitem{gpt3} Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D., Language Models are Few-Shot Learners.2020,  arXiv:2005.14165 
\bibitem{sentiment140} Go, A., Bhayani, R., Huang, L., Twitter Sentiment Classification using Distant Supervision. 2009
\bibitem{twitter} So twitterst du. https://help.twitter.com/de/using-twitter/how-to-tweet
\bibitem{sentiment140} http://help.sentiment140.com/for-students
\bibitem{keras} https://keras.io/
\bibitem{tensorflow} https://www.tensorflow.org/
\bibitem{colab} https://colab.research.google.com/notebooks/intro.ipynb
\end{thebibliography}


\end{document}
