%% Dokumenteinstellungen
\documentclass[12pt,german]{report}

%% Verwendete Pakete
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage{geometry}
\geometry{verbose,a4paper,tmargin=4cm,bmargin=4cm,lmargin=2.8cm,rmargin=2.8cm}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{listings}
\titleformat{\chapter}{\huge}{\thechapter.}{20pt}{\huge}

%% Einstellungen
\setlength{\oddsidemargin}{0cm}
\setlength{\topmargin}{-1.5cm}
\setlength{\textheight}{22cm}
\setlength{\textwidth}{16cm}
\setlength{\headsep}{1cm}
\setlength{\parindent}{0ex}
\setlength{\parskip}{2.0ex plus 0.9ex minus 0.4ex}

\begin{document}

%% Titelblatt
\input{ba_titel.tex}
\newpage 
\thispagestyle{empty}
\quad 
\newpage

%% Inhaltsverzeichnis
\tableofcontents
\newpage 
\thispagestyle{empty}
\quad 
\newpage

%% Formaljuristisches
\input{ba_jurist.tex}
\newpage 
\thispagestyle{empty}
\quad 
\newpage

%% Einzeldokumente, z.B. in Kapitel aufgeteilt

\begin{onehalfspace}
\input{kapitel/kapitel0.tex}
\input{kapitel/kapitel1.tex}
\input{kapitel/kapitel2.tex}
\input{kapitel/kapitel3.tex}
\end{onehalfspace}

%% Abbildungsverzeichnis
\listoffigures

\begin{thebibliography}{}
%% Zitieren Sie diesen ersten Eintrag mit \cite{bib_1}
\bibitem{ulm} Howard, J., Ruder, S., Universal Language Model Fine-tuning for Text Classification. 2018,  arXiv:1801.06146 
\bibitem{ulm_model} Marev, K., Georgiev, K., Automated aviation occurrences categorization. 2019, https://ieeexplore.ieee.org/document/8870055/
\bibitem{attention} Vaswani, A. et al., Attention Is All You Need. 2017,  arXiv:1706.03762v5 
\bibitem{transformerxl} Dai, Z., Yang, Z., Yang,Y., Carbonell, J., Le, Q., Salakhutdinov, R., Transformer-XL: Attentive Language Models
Beyond a Fixed-Length Context. 2019,  arXiv:1901.02860v3 
\bibitem{elmo} Peters, M.,  Neumann, M.,  Iyyer, M., Gardner, M.,  Clark, C. , Lee, K., Zettlemoyer, L.,  Deep contextualized word representations. 2018, arXiv:1802.05365v2
\bibitem{elmoex} Mihail, E., Deep Contextualized Word Representations with ELMo. 2018, https://www.mihaileric.com/posts/deep-contextualized-word-representations-elmo/
\bibitem{bert} Devlin, J., Chang, M., Lee K. , Toutanova, K., BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding. 2019, arXiv:1810.04805v2
\bibitem{roberta} Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V., RoBERTa: A Robustly Optimized BERT Pretraining Approach. 2019, arXiv:1907.11692
\bibitem{gpt} Radford,  A.,  Narasimhan,  K.,  Salimans,  T.,  and  Sutskever,  I., Improving language understanding by generative pre-training. 2018, https://openai.com/blog/language-unsupervised/
\bibitem{gpt2} Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., Language Models are Unsupervised Multitask Learners. 2018, https://openai.com/blog/better-language-models/
\bibitem{gpt3} Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D., Language Models are Few-Shot Learners. 2020,  arXiv:2005.14165 
\bibitem{xlnet} Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., Le, Q., XLNet: Generalized Autoregressive Pretraining
for Language Understanding. 2019, arXiv:1906.08237v2
\bibitem{xlnetex} Kurita, K., XLNet Explained. 2019,  https://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained
\bibitem{t4sa} Vadicamo, L., Carrara, F., Cimino, A., Cresci, S., Dell'Orletta, F., Falchi, F., Tesconi, M., Cross-Media Learning for Image Sentiment Analysis in the Wild. 2017, http://www.t4sa.it/
\bibitem{sentiment140} Go, A., Bhayani, R., Huang, L., Twitter Sentiment Classification using Distant Supervision. 2009
\bibitem{crowdflower} Crowdflower, Apple Twitter Sentiment. 2016, https://data.world/crowdflower/apple-twitter-sentiment
\bibitem{twitter} So twitterst du. https://help.twitter.com/de/using-twitter/how-to-tweet
\bibitem{keras} https://keras.io/
\bibitem{tensorflow} https://www.tensorflow.org/
\bibitem{colab} https://colab.research.google.com/notebooks/intro.ipynb
\bibitem{colab_gpu} https://research.google.com/colaboratory/faq.html
\bibitem{sentiment140_data} http://help.sentiment140.com/for-students
\bibitem{apple_sent} https://www.kaggle.com/seriousran/appletwittersentimenttexts
\bibitem{airlines} https://www.kaggle.com/crowdflower/twitter-airline-sentiment
\end{thebibliography}


\end{document}
