{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLNet for stance detection\n",
    "This notebook shows how to use XLNet for stance detection with two sample datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer,XLNetForSequenceClassification, XLNetConfig\n",
    "from transformers import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader,RandomSampler,SequentialSampler\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports for data cleaning and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files\n",
    "Only execute one of these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5159: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "stances = pd.read_csv('data/stance/train_stances.csv')\n",
    "bodies = pd.read_csv('data/stance/train_bodies.csv')\n",
    "\n",
    "stances = stances.set_index(stances['Body ID']).drop('Body ID', axis = 1)\n",
    "bodies = bodies.set_index(bodies['Body ID']).drop('Body ID', axis = 1)\n",
    "\n",
    "data = stances.join(bodies)\n",
    "\n",
    "data.rename(columns={\"Headline\": \"text_a\", \"articleBody\": \"text_b\", \"Stance\" : \"labels\"}, inplace=True)\n",
    "data = data[[\"text_a\", \"text_b\", \"labels\"]]\n",
    "\n",
    "data.text_b = data.text_b.map(lambda x : x.translate(str.maketrans('', '', string.punctuation)))\n",
    "data.text_a = data.text_a.map(lambda x : x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "data.text_a = data.text_a.str.lower()\n",
    "data.text_b = data.text_b.str.lower()\n",
    "\n",
    "data.text_a = data.text_a.str.strip(string.whitespace)\n",
    "data.text_b = data.text_b.str.strip(string.whitespace)\n",
    "\n",
    "df_train, df_test = train_test_split(data, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "thisdict =\t{\n",
    "  \"unrelated\": 0,\n",
    "  \"agree\": 1,\n",
    "  \"discuss\": 2,\n",
    "  \"disagree\": 3\n",
    "}\n",
    "df_train.labels = df_train.labels.apply(lambda x: thisdict[x])\n",
    "df_test.labels = df_test.labels.apply(lambda x: thisdict[x])\n",
    "\n",
    "df_train.reset_index(inplace=True, drop = True)\n",
    "df_test.reset_index(inplace=True, drop = True)\n",
    "\n",
    "\n",
    "num_stances = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>american bombing had signed my death certifica...</td>\n",
       "      <td>a schoolboy who was almost killed when he was ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>isis border crisis dhs chief says terrorists n...</td>\n",
       "      <td>the pentagon has confirmed that the weapons we...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kevin vickers sergeantatarms shoots a shooting...</td>\n",
       "      <td>two friends have a 20yearold mcdonald’s quarte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that story about a catholic priest dying seein...</td>\n",
       "      <td>absolutely awful news media are reporting that...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>its rubbish that robert plant turned down £500...</td>\n",
       "      <td>robert plant’s publicist has described as “rub...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33476</th>\n",
       "      <td>hbo streaming service could launch in april fo...</td>\n",
       "      <td>cnn  the mystery surrounding north koreas erra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33477</th>\n",
       "      <td>news youll never guess how a homeless man spen...</td>\n",
       "      <td>you know that tendency we have to judge people...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33478</th>\n",
       "      <td>hostage david haines murder evil pm says</td>\n",
       "      <td>when a report went viral that nbc meteorologis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33479</th>\n",
       "      <td>isis leader dead</td>\n",
       "      <td>new delhi ak verma an executive engineer at th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33480</th>\n",
       "      <td>best christmas present ever heartwarming momen...</td>\n",
       "      <td>iraqi pilots who have joined islamic state are...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33481 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text_a  \\\n",
       "0      american bombing had signed my death certifica...   \n",
       "1      isis border crisis dhs chief says terrorists n...   \n",
       "2      kevin vickers sergeantatarms shoots a shooting...   \n",
       "3      that story about a catholic priest dying seein...   \n",
       "4      its rubbish that robert plant turned down £500...   \n",
       "...                                                  ...   \n",
       "33476  hbo streaming service could launch in april fo...   \n",
       "33477  news youll never guess how a homeless man spen...   \n",
       "33478           hostage david haines murder evil pm says   \n",
       "33479                                   isis leader dead   \n",
       "33480  best christmas present ever heartwarming momen...   \n",
       "\n",
       "                                                  text_b  labels  \n",
       "0      a schoolboy who was almost killed when he was ...       0  \n",
       "1      the pentagon has confirmed that the weapons we...       0  \n",
       "2      two friends have a 20yearold mcdonald’s quarte...       0  \n",
       "3      absolutely awful news media are reporting that...       0  \n",
       "4      robert plant’s publicist has described as “rub...       1  \n",
       "...                                                  ...     ...  \n",
       "33476  cnn  the mystery surrounding north koreas erra...       0  \n",
       "33477  you know that tendency we have to judge people...       1  \n",
       "33478  when a report went viral that nbc meteorologis...       0  \n",
       "33479  new delhi ak verma an executive engineer at th...       0  \n",
       "33480  iraqi pilots who have joined islamic state are...       0  \n",
       "\n",
       "[33481 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/stance/trainingdata-all-annotations.txt', delimiter='\\t', encoding='iso8859-2')\n",
    "df_test = pd.read_csv('data/stance/testdata-taskA-all-annotations.txt', delimiter='\\t'\n",
    "                     ).append(pd.read_csv('data/stance/testdata-taskB-all-annotations.txt', delimiter='\\t'))\n",
    "\n",
    "\n",
    "df_train.drop(['ID', 'Opinion towards', 'Sentiment'], axis=1, inplace=True)\n",
    "df_test.drop(['ID', 'Opinion towards', 'Sentiment'], axis = 1, inplace=True)\n",
    "\n",
    "df_train.rename(columns={\"Target\": \"text_a\", \"Tweet\": \"text_b\", \"Stance\" : \"labels\"}, inplace=True)\n",
    "df_test.rename(columns={\"Target\": \"text_a\", \"Tweet\": \"text_b\", \"Stance\" : \"labels\"}, inplace=True)\n",
    "\n",
    "df_train.text_b = df_train.text_b.apply(lambda x:re.sub(r'http\\S+', '', x))\n",
    "df_test.text_b = df_test.text_b.apply(lambda x:re.sub(r'http\\S+', '', x))\n",
    "\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "df_train.text_b = df_train.text_b.apply(lambda x: tokenizer.tokenize(x))\n",
    "df_test.text_b = df_test.text_b.apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "df_train.text_b = df_train.text_b.apply(lambda x: ' '.join(x))\n",
    "df_test.text_b = df_test.text_b.apply(lambda x: ' '.join(x))\n",
    "\n",
    "df_train.text_b = df_train.text_b.map(lambda x : x.translate(str.maketrans('', '', string.punctuation)))\n",
    "df_test.text_b = df_test.text_b.map(lambda x : x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "df_train.text_a = df_train.text_a.str.lower()\n",
    "df_train.text_b = df_train.text_b.str.lower()\n",
    "df_test.text_a = df_test.text_a.str.lower()\n",
    "df_test.text_b = df_test.text_b.str.lower()\n",
    "\n",
    "df_train.text_b = df_train.text_b.str.strip(string.whitespace)\n",
    "df_test.text_b = df_test.text_b.str.strip(string.whitespace)\n",
    "\n",
    "\n",
    "df_train.reset_index(inplace=True, drop = True)\n",
    "df_test.reset_index(inplace=True, drop = True)\n",
    "\n",
    "\n",
    "thisdict =\t{\n",
    "  \"AGAINST\": 0,\n",
    "  \"FAVOR\": 1,\n",
    "  \"NONE\": 2\n",
    "}\n",
    "df_train.labels = df_train.labels.apply(lambda x: thisdict[x])\n",
    "df_test.labels = df_test.labels.apply(lambda x: thisdict[x])\n",
    "\n",
    "num_stances = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2814"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Only execute with non-simple Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train  = []\n",
    "for i in range(len(df_train)):\n",
    "  sentence = df_train.text_a[i]+\"[SEP]\"+df_train.text_b[i]+\"[CLS]\"\n",
    "  sentences_train.append(sentence)\n",
    "    \n",
    "sentences_test  = []\n",
    "for i in range(len(df_test)):\n",
    "  sentence = df_test.text_a[i]+\"[SEP]\"+df_test.text_b[i]+\"[CLS]\"\n",
    "  sentences_test.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer  = XLNetTokenizer.from_pretrained('xlnet-base-cased',do_lower_case=True)\n",
    "tokenized_text_train = [tokenizer.tokenize(sent) for sent in sentences_train]\n",
    "tokenized_text_test = [tokenizer.tokenize(sent) for sent in sentences_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text_train]\n",
    "ids_test = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = df_train['labels'].values\n",
    "\n",
    "labels_test = df_test['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max1 = len(ids_train[0])\n",
    "for i in ids_train:\n",
    "  if(len(i)>max1):\n",
    "    max1=len(i)\n",
    "    \n",
    "MAX_LEN_TRAIN = max1\n",
    "\n",
    "max1 = len(ids_test[0])\n",
    "for i in ids_test:\n",
    "  if(len(i)>max1):\n",
    "    max1=len(i)\n",
    "    \n",
    "MAX_LEN_TEST = max1\n",
    "\n",
    "if (MAX_LEN_TEST > MAX_LEN_TRAIN):\n",
    "    MAX_LEN = MAX_LEN_TEST \n",
    "else :\n",
    "    MAX_LEN = MAX_LEN_TRAIN\n",
    "    \n",
    "    \n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train2 = pad_sequences(ids_train,maxlen=MAX_LEN,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
    "input_ids_test2 = pad_sequences(ids_test,maxlen=MAX_LEN,dtype=\"long\",truncating=\"post\",padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = input_ids_train2\n",
    "xtest = input_ids_test2\n",
    "ytrain = labels_train\n",
    "ytest = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = torch.tensor(xtrain)\n",
    "Ytrain = torch.tensor(ytrain)\n",
    "Xtest = torch.tensor(xtest)\n",
    "Ytest = torch.tensor(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(Xtrain,Ytrain)\n",
    "test_data = TensorDataset(Xtest,Ytest)\n",
    "loader = DataLoader(train_data,batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = XLNetConfig.from_pretrained('xlnet-base-cased')\n",
    "# Set number of output labels\n",
    "config.num_labels = num_stances\n",
    "config.n_gpu=16\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XLNetForSequenceClassification(config)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),lr=2e-5)# We pass model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds,labels):  # A function to predict Accuracy\n",
    "  correct=0\n",
    "  for i in range(0,len(labels)):\n",
    "    if(preds[i]==labels[i]):\n",
    "      correct+=1\n",
    "  return (correct/len(labels))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (non simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_train = 0\n",
    "#Change epochs for training duration\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "  loss1 = []\n",
    "  steps = 0\n",
    "  train_loss = []\n",
    "  l = []\n",
    "  for inputs,labels1 in loader :\n",
    "    inputs.to(device)\n",
    "    labels1.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs.to(device))\n",
    "    loss = criterion(outputs[0],labels1.to(device)).to(device)\n",
    "    logits = torch.max(outputs[0], 1)[1]\n",
    "    #ll=outp(loss)\n",
    "    [train_loss.append(p.item()) for p in torch.argmax(outputs[0],axis=1).flatten() ]#our predicted \n",
    "    [l.append(z.item()) for z in labels1]# real labels\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss1.append(loss.item())\n",
    "    no_train += inputs.size(0)\n",
    "    steps += 1\n",
    "  print(\"Current Loss is : {} Step is : {} number of Example : {} Accuracy : {}\".format(loss.item(),epoch,no_train,flat_accuracy(train_loss,l)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "for inp,lab1 in test_loader:\n",
    "  inp.to(device)\n",
    "  lab1.to(device)\n",
    "  outp1 = model(inp.to(device))\n",
    "  _, pred_label = torch.max(outp1[0], 1)\n",
    "  [predictions.append(p1.item()) for p1 in torch.argmax(outp1[0],1).flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.f1_score(labels_test, predictions, average=None))\n",
    "print(metrics.accuracy_score(labels_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SimpleTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a31c4416aa046dd9bfc5b3ce6035208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=33481.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2318dc839f5462080f1ffe8028ce268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=10.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de53bbeb2d3d4e948d2d70beda3f140f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0', max=2093.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.021631"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.013357"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.505795\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0014733c89b4cb7ac2fb0083fa1c77f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 1', max=2093.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.033231\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6045c72b999647ecae8ebdd2c7b3c969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 2', max=2093.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.008157\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d51daaa83f4f61b86a9b22c08b4d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 3', max=2093.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.000790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61db3c7f60094475b418bf0416ea9e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 4', max=2093.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.000085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e9511bff864fcb8b5a3a2d0032254a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 5', max=2093.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.000201\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1bf5a4d8c846968e65cf054018f1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 6', max=2093.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.000041\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11c36249740402ca03f789b2dbfe40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 7', max=2093.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.000016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3341b0e0a0c54530830a7358c27f6b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 8', max=2093.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.000011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85af0956693b46ba8435c1ccc944ef2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 9', max=2093.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.000026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = ClassificationModel('xlnet', 'xlnet-base-cased', num_labels=num_stances, use_cuda=True, args={\n",
    "    'learning_rate':3e-5,\n",
    "    'num_train_epochs': 10,\n",
    "    'reprocess_input_data': True,\n",
    "    'overwrite_output_dir': True,\n",
    "    'process_count': 10,\n",
    "    'train_batch_size': 16,\n",
    "    'eval_batch_size': 16,\n",
    "    'max_seq_length': 512,\n",
    "    'n_gpu' : 16,\n",
    "    'fp16': False\n",
    "})\n",
    "\n",
    "model.train_model(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e9fdec841c4d61810817f347748655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16491.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77f9354bf8441d4ba26cf46d424442c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Evaluation', max=1031.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/modeling_xlnet.py:272: UserWarning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorIterator.cpp:918.)\n",
      "  attn_score = (ac + bd + ef) * self.scale\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "_, model_outputs_test, _ = model.eval_model(df_test)\n",
    "\n",
    "preds_test = np.argmax(model_outputs_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9985844  0.97600651 0.98633333 0.96633663]\n",
      "0.9941786428961251\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "print(f1_score(df_test.labels, preds_test, average=None))\n",
    "print(accuracy_score(df_test.labels, preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
