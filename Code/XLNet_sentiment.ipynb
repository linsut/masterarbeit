{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLNet for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Xeb1MHq-oXCA"
   },
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer,XLNetForSequenceClassification, XLNetConfig\n",
    "from transformers import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader,RandomSampler,SequentialSampler\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files\n",
    "Only execute one of these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6s0OEaHoXDi"
   },
   "source": [
    "## Apple Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/sentiment/datasets_652925_1154930_apple-twitter-sentiment-texts.csv\")\n",
    "\n",
    "data.sentiment = data.sentiment.apply(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Airline Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/sentiment/Tweets.csv\")\n",
    "data = data[['text', 'airline_sentiment']]\n",
    "data.rename(columns={'airline_sentiment' : 'sentiment'}, inplace=True)\n",
    "\n",
    "thisdict =\t{\n",
    "  \"negative\": 0,\n",
    "  \"neutral\": 1,\n",
    "  \"positive\": 2\n",
    "}\n",
    "data.sentiment = data.sentiment.apply(lambda x: thisdict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T4SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"data/sentiment/raw_tweets_text.csv\")\n",
    "sentiments = pd.read_csv(\"data/sentiment/t4sa_text_sentiment.csv\",delimiter = \"\\t\")\n",
    "\n",
    "tweets.set_index(tweets.id, inplace=True)\n",
    "sentiments.set_index(sentiments.TWID, inplace=True)\n",
    "data=tweets.join(sentiments)\n",
    "data.dropna(inplace=True)\n",
    "data.drop(columns=['id', 'TWID'], inplace=True)\n",
    "data[\"sentiment\"] = data[['NEU', 'NEG', 'POS']].idxmax(axis=1)\n",
    "\n",
    "data = data[['text', 'sentiment']]\n",
    "\n",
    "thisdict =\t{\n",
    "  \"NEG\": 0,\n",
    "  \"NEU\": 1,\n",
    "  \"POS\": 2\n",
    "}\n",
    "data.sentiment = data.sentiment.apply(lambda x: thisdict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text = data.text.str.lower()\n",
    "\n",
    "data.text = data.text.apply(lambda x:re.sub(r'http\\S+', '', x))\n",
    "\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "data.text = data.text.apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "data.text = data.text.apply(lambda x: ' '.join(x))\n",
    "\n",
    "data.text = data.text.map(lambda x : x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "data.text = data.text.str.replace(\"[0-9]\", \" \")\n",
    "\n",
    "data.text = data.text.str.strip(string.whitespace)\n",
    "\n",
    "df_train, df_test = train_test_split(data, test_size=0.33, random_state=42)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "J2BADEhO-Q5n",
    "outputId": "a9e6359b-7b5e-493b-a697-350d62367a75"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we need more products like  companies like    ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>legit thought that was you in this pic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aapl rt alex gauna flips his apple bit  sets s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>here are the main differences between apple ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trade  aapl free nightly updates are posted he...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>i kinda feel sorry for tho  goodjob with your ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>your ipad game just got shut way down  like   ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>been waiting   days so far for to approve the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>my tmobile  apple cellular hell  iphone  iphon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>rt  why the fuck is there a  recently deleted ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1092 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  sentiment\n",
       "0     we need more products like  companies like    ...          1\n",
       "1                legit thought that was you in this pic          1\n",
       "2     aapl rt alex gauna flips his apple bit  sets s...          1\n",
       "3     here are the main differences between apple ca...          1\n",
       "4     trade  aapl free nightly updates are posted he...          1\n",
       "...                                                 ...        ...\n",
       "1087  i kinda feel sorry for tho  goodjob with your ...          0\n",
       "1088  your ipad game just got shut way down  like   ...          0\n",
       "1089  been waiting   days so far for to approve the ...          0\n",
       "1090  my tmobile  apple cellular hell  iphone  iphon...          0\n",
       "1091  rt  why the fuck is there a  recently deleted ...          0\n",
       "\n",
       "[1092 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aI3TM-V7oXEu"
   },
   "source": [
    "# Tokenization\n",
    "Only execute with non-simple Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RE440k7s-cEz"
   },
   "outputs": [],
   "source": [
    "sentences_train  = []\n",
    "for sentence in df_train['text']:\n",
    "  sentence = sentence+\"[SEP] [CLS]\"\n",
    "  sentences_train.append(sentence)\n",
    "    \n",
    "sentences_test  = []\n",
    "for sentence in df_test['text']:\n",
    "  sentence = sentence+\"[SEP] [CLS]\"\n",
    "  sentences_test.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NP1e3cNs-g3v",
    "outputId": "c51aa8b1-78c6-42a2-b36a-2853f9d5668c"
   },
   "outputs": [],
   "source": [
    "sentences_train[0] ##To check if tags are added or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfPGi_EIoXHY"
   },
   "source": [
    "### Inputs\n",
    "\n",
    "1. XLNet tokenizer is used to convert our text into tokens that correspond to   XLNet’s vocabulary.\n",
    "2. a sequence of integers identifying each input token to its index number in the XLNet tokenizer \n",
    "    - Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lwqJ212uAtck"
   },
   "outputs": [],
   "source": [
    "tokenizer  = XLNetTokenizer.from_pretrained('xlnet-base-cased',do_lower_case=True)\n",
    "tokenized_text_train = [tokenizer.tokenize(sent) for sent in sentences_train]\n",
    "tokenized_text_test = [tokenizer.tokenize(sent) for sent in sentences_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "y4O23uMZBN6K",
    "outputId": "b28dc371-defc-4581-dc7a-8522dba79b45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁for',\n",
       " '▁all',\n",
       " '▁who',\n",
       " '▁served',\n",
       " '▁and',\n",
       " '▁fought',\n",
       " '▁for',\n",
       " '▁our',\n",
       " '▁country',\n",
       " '▁we',\n",
       " '▁thank',\n",
       " '▁you',\n",
       " '▁salute',\n",
       " 'to',\n",
       " 'service',\n",
       " '▁veterans',\n",
       " 'day',\n",
       " '[',\n",
       " 's',\n",
       " 'ep',\n",
       " ']',\n",
       " '▁[',\n",
       " 'cl',\n",
       " 's',\n",
       " ']']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ie341hRDB6T4"
   },
   "outputs": [],
   "source": [
    "ids_train = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text_train]\n",
    "ids_test = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OK-UXNt5CeTN",
    "outputId": "cfea254a-d08d-4188-cea4-d09b01055282"
   },
   "outputs": [],
   "source": [
    "\n",
    "labels_train = df_train.sentiment.values\n",
    "\n",
    "labels_test = df_test.sentiment.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONrApxqmoXJN"
   },
   "source": [
    " We find the maximum length of our sentences so that we can pad the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MYx9P-pZFrU6",
    "outputId": "56c7aa88-173c-42cd-f86c-b621c6b75385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    }
   ],
   "source": [
    "max1 = len(ids_train[0])\n",
    "for i in ids_train:\n",
    "  if(len(i)>max1):\n",
    "    max1=len(i)\n",
    "    \n",
    "MAX_LEN_TRAIN = max1\n",
    "\n",
    "max1 = len(ids_test[0])\n",
    "for i in ids_test:\n",
    "  if(len(i)>max1):\n",
    "    max1=len(i)\n",
    "    \n",
    "MAX_LEN_TEST = max1\n",
    "\n",
    "if (MAX_LEN_TEST > MAX_LEN_TRAIN):\n",
    "    MAX_LEN = MAX_LEN_TEST \n",
    "else :\n",
    "    MAX_LEN = MAX_LEN_TRAIN\n",
    "    \n",
    "    \n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5211mavoXJt"
   },
   "source": [
    "We pad our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Sz7Y0D3xGKaw"
   },
   "outputs": [],
   "source": [
    "input_ids_train2 = pad_sequences(ids_train,maxlen=MAX_LEN,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
    "input_ids_test2 = pad_sequences(ids_test,maxlen=MAX_LEN,dtype=\"long\",truncating=\"post\",padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "A8KPBEzPCgo9"
   },
   "outputs": [],
   "source": [
    "xtrain = input_ids_train2\n",
    "xtest = input_ids_test2\n",
    "ytrain = labels_train\n",
    "ytest = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YSjgfMLxDCyi"
   },
   "outputs": [],
   "source": [
    "Xtrain = torch.tensor(xtrain)\n",
    "Ytrain = torch.tensor(ytrain)\n",
    "Xtest = torch.tensor(xtest)\n",
    "Ytest = torch.tensor(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dj1ef2DNFHLc"
   },
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hrTbjxhhHsDc"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(Xtrain,Ytrain)\n",
    "test_data = TensorDataset(Xtest,Ytest)\n",
    "loader = DataLoader(train_data,batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetConfig {\n",
       "  \"architectures\": [\n",
       "    \"XLNetLMHeadModel\"\n",
       "  ],\n",
       "  \"attn_type\": \"bi\",\n",
       "  \"bi_data\": false,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"clamp_len\": -1,\n",
       "  \"d_head\": 64,\n",
       "  \"d_inner\": 3072,\n",
       "  \"d_model\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"end_n_top\": 5,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"ff_activation\": \"gelu\",\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"mem_len\": null,\n",
       "  \"model_type\": \"xlnet\",\n",
       "  \"n_head\": 12,\n",
       "  \"n_layer\": 12,\n",
       "  \"pad_token_id\": 5,\n",
       "  \"reuse_len\": null,\n",
       "  \"same_length\": false,\n",
       "  \"start_n_top\": 5,\n",
       "  \"summary_activation\": \"tanh\",\n",
       "  \"summary_last_dropout\": 0.1,\n",
       "  \"summary_type\": \"last\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 250\n",
       "    }\n",
       "  },\n",
       "  \"untie_r\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = XLNetConfig.from_pretrained('xlnet-base-cased')\n",
    "# Set number of output labels\n",
    "config.num_labels = 3\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9-ELFF0PIr81",
    "outputId": "bcb5db9c-50b1-423c-817a-fa699d3c7ce7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassification(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sequence_summary): SequenceSummary(\n",
       "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (first_dropout): Identity()\n",
       "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (logits_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XLNetForSequenceClassification(config)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wKPtP7zaLShu"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),lr=2e-5)# We pass model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LXEydZNIm4-J"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZL4HkhCLcff"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def flat_accuracy(preds,labels):  # A function to predict Accuracy\n",
    "  correct=0\n",
    "  for i in range(0,len(labels)):\n",
    "    if(preds[i]==labels[i]):\n",
    "      correct+=1\n",
    "  return (correct/len(labels))*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIaGghHQoXPN"
   },
   "source": [
    "# Training (non simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Tfr3IOEoSymH",
    "outputId": "8158327b-1eed-4679-a93b-000b3812ef77"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e72d2de3f011>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m# real labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mloss1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mno_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"correct_bias\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# No bias correction for Bert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                     \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "no_train = 0\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "  loss1 = []\n",
    "  steps = 0\n",
    "  train_loss = []\n",
    "  l = []\n",
    "  for inputs,labels1 in loader :\n",
    "    inputs.to(device)\n",
    "    labels1.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs.to(device))\n",
    "    loss = criterion(outputs[0],labels1.to(device)).to(device)\n",
    "    logits = torch.max(outputs[0], 1)[1]\n",
    "    #ll=outp(loss)\n",
    "    [train_loss.append(p.item()) for p in torch.argmax(outputs[0],axis=1).flatten() ]#our predicted \n",
    "    [l.append(z.item()) for z in labels1]# real labels\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss1.append(loss.item())\n",
    "    no_train += inputs.size(0)\n",
    "    steps += 1\n",
    "  print(\"Current Loss is : {} Step is : {} number of Example : {} Accuracy : {}\".format(loss.item(),epoch,no_train,flat_accuracy(train_loss,l)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbM73sOCoXPl"
   },
   "source": [
    "- torch.argmax() returns the index of the max number \n",
    "- axis = 1 means that it will search maximum number in a row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "for inp,lab1 in test_loader:\n",
    "  inp.to(device)\n",
    "  lab1.to(device)\n",
    "  outp1 = model(inp.to(device))\n",
    "  _, pred_label = torch.max(outp1[0], 1)\n",
    "  [predictions.append(p1.item()) for p1 in torch.argmax(outp1[0],1).flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.f1_score(labels_test, predictions, average=None))\n",
    "print(metrics.accuracy_score(labels_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SimpleTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/simpletransformers/classification/classification_model.py:270: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509253640a314ee8b88d0d5a3f402e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1092.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c389c2c8dab344cfa6eac32713fc8b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=5.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473e09fd3c7a44a7b2c5684002bca881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0', max=35.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/modeling_xlnet.py:272: UserWarning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorIterator.cpp:918.)\n",
      "  attn_score = (ac + bd + ef) * self.scale\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.289040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78cfbd7082ef41dc96c567a8b5cc7f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 1', max=35.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.618159"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.061754\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a8360c29c5472ba87f2469e9e9ea1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 2', max=35.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.077173\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13a636df3224e62999a398e82ef85bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 3', max=35.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.717344\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cca569dafe64c43829fb58796436b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 4', max=35.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.780357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "model = ClassificationModel('xlnet', 'xlnet-base-cased', num_labels=3, use_cuda=True, args={\n",
    "    'learning_rate':3e-5,\n",
    "    'num_train_epochs': 5,\n",
    "    'reprocess_input_data': True,\n",
    "    'overwrite_output_dir': True,\n",
    "    'process_count': 10,\n",
    "    'train_batch_size': 32,\n",
    "    'eval_batch_size': 32,\n",
    "    'max_seq_length': 512,\n",
    "    'n_gpu' : 16,\n",
    "    'fp16': False\n",
    "})\n",
    "\n",
    "model.train_model(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/simpletransformers/classification/classification_model.py:691: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9ab0f6b7104ec7a98c49942e8f1785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=538.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f888b517db644a799369e70162ad427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Evaluation', max=17.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "_, model_outputs_test, _ = model.eval_model(df_test)\n",
    "\n",
    "preds_test = np.argmax(model_outputs_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87719298 0.87867647 0.63157895]\n",
      "0.8605947955390335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "print(f1_score(df_test.sentiment, preds_test, average=None))\n",
    "print(accuracy_score(df_test.sentiment, preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what said</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plus youve added commercials to the experience...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i didnt today  must mean i need to take anothe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>its really aggressive to blast obnoxious  ente...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and its a really big bad thing about it</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>thank you we got on a different flight to chicago</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>leaving over    minutes late flight  no warnin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>please bring american airlines to blackberry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>you have my money  you change my flight  and d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>we have   ppl so we need   know how many seats...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "0                                              what said          1\n",
       "1      plus youve added commercials to the experience...          2\n",
       "2      i didnt today  must mean i need to take anothe...          1\n",
       "3      its really aggressive to blast obnoxious  ente...          0\n",
       "4                and its a really big bad thing about it          0\n",
       "...                                                  ...        ...\n",
       "14635  thank you we got on a different flight to chicago          2\n",
       "14636  leaving over    minutes late flight  no warnin...          0\n",
       "14637       please bring american airlines to blackberry          1\n",
       "14638  you have my money  you change my flight  and d...          0\n",
       "14639  we have   ppl so we need   know how many seats...          1\n",
       "\n",
       "[14640 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
