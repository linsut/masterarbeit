{"cells":[{"metadata":{"id":"Xeb1MHq-oXCA","trusted":true},"cell_type":"code","source":"!pip install pytorch-transformers\n\nimport pandas as pd\nfrom transformers import XLNetTokenizer,XLNetForSequenceClassification, XLNetConfig\nfrom transformers import AdamW\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.sequence import pad_sequences\nimport torch\nfrom torch.utils.data import TensorDataset,DataLoader,RandomSampler,SequentialSampler\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Einlesen","execution_count":null},{"metadata":{"id":"T6s0OEaHoXDi"},"cell_type":"markdown","source":"Apple Sentiment","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport pandas as pd\nimport re\n\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv(\"../input/appletwittersentimenttexts/apple-twitter-sentiment-texts.csv\")\ndf_train, df_test = train_test_split(data, test_size=0.33, random_state=42)\n\ndf_train.text = df_train.text.str.lower()\ndf_test.text = df_test.text.str.lower()\n\ndf_train.text = df_train.text.apply(lambda x:re.sub(r'http\\S+', '', x))\ndf_test.text = df_test.text.apply(lambda x:re.sub(r'http\\S+', '', x))\n\ntokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\ndf_train.text = df_train.text.apply(lambda x: tokenizer.tokenize(x))\ndf_test.text = df_test.text.apply(lambda x: tokenizer.tokenize(x))\n\ndf_train.text = df_train.text.apply(lambda x: ' '.join(x))\ndf_test.text = df_test.text.apply(lambda x: ' '.join(x))\n\ndf_train.text = df_train.text.map(lambda x : x.translate(str.maketrans('', '', string.punctuation)))\ndf_test.text = df_test.text.map(lambda x : x.translate(str.maketrans('', '', string.punctuation)))\n\ndf_train.text = df_train.text.str.replace(\"[0-9]\", \" \")\ndf_test.text = df_test.text.str.replace(\"[0-9]\", \" \")\n\ndf_train.text = df_train.text.str.strip(string.whitespace)\ndf_test.text = df_test.text.str.strip(string.whitespace)\n\ndf_train = df_train.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\ndf_train.sentiment = df_train.sentiment.apply(lambda x: x + 1)\ndf_test.sentiment = df_test.sentiment.apply(lambda x: x + 1)\n\nsentiment_column = 'sentiment'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"US Airline Sentiment","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport pandas as pd\nimport re\n\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv(\"../input/twitter-airline-sentiment/Tweets.csv\")\ndata = data[['text', 'airline_sentiment']].copy()\ndf_train, df_test = train_test_split(data, test_size=0.33, random_state=42)\n\ndf_train.text = df_train.text.str.lower()\ndf_test.text = df_test.text.str.lower()\n\ndf_train.text = df_train.text.apply(lambda x:re.sub(r'http\\S+', '', x))\ndf_test.text = df_test.text.apply(lambda x:re.sub(r'http\\S+', '', x))\n\ntokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\ndf_train.text = df_train.text.apply(lambda x: tokenizer.tokenize(x))\ndf_test.text = df_test.text.apply(lambda x: tokenizer.tokenize(x))\n\ndf_train.text = df_train.text.apply(lambda x: ' '.join(x))\ndf_test.text = df_test.text.apply(lambda x: ' '.join(x))\n\ndf_train.text = df_train.text.map(lambda x : x.translate(str.maketrans('', '', string.punctuation)))\ndf_test.text = df_test.text.map(lambda x : x.translate(str.maketrans('', '', string.punctuation)))\n\ndf_train.text = df_train.text.str.replace(\"[0-9]\", \" \")\ndf_test.text = df_test.text.str.replace(\"[0-9]\", \" \")\n\ndf_train.text = df_train.text.str.strip(string.whitespace)\ndf_test.text = df_test.text.str.strip(string.whitespace)\n\ndf_train = df_train.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\nthisdict =\t{\n  \"negative\": 0,\n  \"neutral\": 1,\n  \"positive\": 2\n}\ndf_train.airline_sentiment = df_train.airline_sentiment.apply(lambda x: thisdict[x])\ndf_test.airline_sentiment = df_test.airline_sentiment.apply(lambda x: thisdict[x])\nsentiment_column = 'airline_sentiment'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"T4SA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport pandas as pd\nimport re\n\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.model_selection import train_test_split\n\ntweets = pd.read_csv(\"../input/t4sa-sentiment-analysis/raw_tweets_text.csv\")\nsentiments = pd.read_csv(\"../input/t4sa-sentiment-analysis/t4sa_text_sentiment.csv\",delimiter = \"\\t\")\n\ntweets.set_index(tweets.id, inplace=True)\nsentiments.set_index(sentiments.TWID, inplace=True)\ndata=tweets.join(sentiments)\ndata.dropna(inplace=True)\ndata.drop(columns=['id', 'TWID'], inplace=True)\ndata[\"sentiment\"] = data[['NEU', 'NEG', 'POS']].idxmax(axis=1)\n\ndata = data[['text', 'sentiment']].copy()\ndf_train, df_test = train_test_split(data, test_size=0.33, random_state=42)\n\ndf_train.text = df_train.text.str.lower()\ndf_test.text = df_test.text.str.lower()\n\ndf_train.text = df_train.text.apply(lambda x:re.sub(r'http\\S+', '', x))\ndf_test.text = df_test.text.apply(lambda x:re.sub(r'http\\S+', '', x))\n\ntokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\ndf_train.text = df_train.text.apply(lambda x: tokenizer.tokenize(x))\ndf_test.text = df_test.text.apply(lambda x: tokenizer.tokenize(x))\n\ndf_train.text = df_train.text.apply(lambda x: ' '.join(x))\ndf_test.text = df_test.text.apply(lambda x: ' '.join(x))\n\ndf_train.text = df_train.text.map(lambda x : x.translate(str.maketrans('', '', string.punctuation)))\ndf_test.text = df_test.text.map(lambda x : x.translate(str.maketrans('', '', string.punctuation)))\n\ndf_train.text = df_train.text.str.replace(\"[0-9]\", \" \")\ndf_test.text = df_test.text.str.replace(\"[0-9]\", \" \")\n\ndf_train.text = df_train.text.str.strip(string.whitespace)\ndf_test.text = df_test.text.str.strip(string.whitespace)\n\ndf_train = df_train.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\nthisdict =\t{\n  \"NEG\": 0,\n  \"NEU\": 1,\n  \"POS\": 2\n}\ndf_train.sentiment = df_train.sentiment.apply(lambda x: thisdict[x])\ndf_test.sentiment = df_test.sentiment.apply(lambda x: thisdict[x])\n\nsentiment_column = 'sentiment'","execution_count":null,"outputs":[]},{"metadata":{"id":"J2BADEhO-Q5n","outputId":"a9e6359b-7b5e-493b-a697-350d62367a75","trusted":true},"cell_type":"code","source":"df_train.sentiment.unique()","execution_count":null,"outputs":[]},{"metadata":{"id":"aI3TM-V7oXEu"},"cell_type":"markdown","source":"- XLNet need <code>[SEP] [CLS]</code> tags at the end of each sentence  \n- We add them by using following code","execution_count":null},{"metadata":{"id":"RE440k7s-cEz","trusted":true},"cell_type":"code","source":"sentences_train  = []\nfor sentence in df_train['text']:\n  sentence = sentence+\"[SEP] [CLS]\"\n  sentences_train.append(sentence)\n    \nsentences_test  = []\nfor sentence in df_test['text']:\n  sentence = sentence+\"[SEP] [CLS]\"\n  sentences_test.append(sentence)","execution_count":null,"outputs":[]},{"metadata":{"id":"NP1e3cNs-g3v","outputId":"c51aa8b1-78c6-42a2-b36a-2853f9d5668c","trusted":true},"cell_type":"code","source":"sentences_train[0] ##To check if tags are added or not","execution_count":null,"outputs":[]},{"metadata":{"id":"PfPGi_EIoXHY"},"cell_type":"markdown","source":"### Inputs\n\n1. XLNet tokenizer is used to convert our text into tokens that correspond to   XLNetâ€™s vocabulary.\n2. a sequence of integers identifying each input token to its index number in the XLNet tokenizer \n    - Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n","execution_count":null},{"metadata":{"id":"lwqJ212uAtck","trusted":true},"cell_type":"code","source":"tokenizer  = XLNetTokenizer.from_pretrained('xlnet-base-cased',do_lower_case=True)\ntokenized_text_train = [tokenizer.tokenize(sent) for sent in sentences_train]\ntokenized_text_test = [tokenizer.tokenize(sent) for sent in sentences_test]","execution_count":null,"outputs":[]},{"metadata":{"id":"y4O23uMZBN6K","outputId":"b28dc371-defc-4581-dc7a-8522dba79b45","trusted":true},"cell_type":"code","source":"tokenized_text_train[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"Ie341hRDB6T4","trusted":true},"cell_type":"code","source":"ids_train = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text_train]\nids_test = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text_test]","execution_count":null,"outputs":[]},{"metadata":{"id":"OK-UXNt5CeTN","outputId":"cfea254a-d08d-4188-cea4-d09b01055282","trusted":true},"cell_type":"code","source":"\nlabels_train = df_train[sentiment_column].values\n\nlabels_test = df_test[sentiment_column].values","execution_count":null,"outputs":[]},{"metadata":{"id":"ONrApxqmoXJN"},"cell_type":"markdown","source":"### We find the maximum length of our sentences so that we can pad the rest","execution_count":null},{"metadata":{"id":"MYx9P-pZFrU6","outputId":"56c7aa88-173c-42cd-f86c-b621c6b75385","trusted":true},"cell_type":"code","source":"max1 = len(ids_train[0])\nfor i in ids_train:\n  if(len(i)>max1):\n    max1=len(i)\n    \nMAX_LEN_TRAIN = max1\n\nmax1 = len(ids_test[0])\nfor i in ids_test:\n  if(len(i)>max1):\n    max1=len(i)\n    \nMAX_LEN_TEST = max1\n\nif (MAX_LEN_TEST > MAX_LEN_TRAIN):\n    MAX_LEN = MAX_LEN_TEST \nelse :\n    MAX_LEN = MAX_LEN_TRAIN\n    \n    \nprint(MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"id":"L5211mavoXJt"},"cell_type":"markdown","source":"#### We pad our sentences","execution_count":null},{"metadata":{"id":"Sz7Y0D3xGKaw","trusted":true},"cell_type":"code","source":"input_ids_train2 = pad_sequences(ids_train,maxlen=MAX_LEN,dtype=\"long\",truncating=\"post\",padding=\"post\")\ninput_ids_test2 = pad_sequences(ids_test,maxlen=MAX_LEN,dtype=\"long\",truncating=\"post\",padding=\"post\")","execution_count":null,"outputs":[]},{"metadata":{"id":"A8KPBEzPCgo9","trusted":true},"cell_type":"code","source":"xtrain = input_ids_train2\nxtest = input_ids_test2\nytrain = labels_train\nytest = labels_test","execution_count":null,"outputs":[]},{"metadata":{"id":"YSjgfMLxDCyi","trusted":true},"cell_type":"code","source":"Xtrain = torch.tensor(xtrain)\nYtrain = torch.tensor(ytrain)\nXtest = torch.tensor(xtest)\nYtest = torch.tensor(ytest)","execution_count":null,"outputs":[]},{"metadata":{"id":"dj1ef2DNFHLc","trusted":true},"cell_type":"code","source":"batch_size = 10","execution_count":null,"outputs":[]},{"metadata":{"id":"hrTbjxhhHsDc","trusted":true},"cell_type":"code","source":"train_data = TensorDataset(Xtrain,Ytrain)\ntest_data = TensorDataset(Xtest,Ytest)\nloader = DataLoader(train_data,batch_size=batch_size)\ntest_loader = DataLoader(test_data,batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = XLNetConfig.from_pretrained('xlnet-base-cased')\n# Set number of output labels\nconfig.num_labels = 3\nconfig","execution_count":null,"outputs":[]},{"metadata":{"id":"9-ELFF0PIr81","outputId":"bcb5db9c-50b1-423c-817a-fa699d3c7ce7","trusted":true},"cell_type":"code","source":"model = XLNetForSequenceClassification(config)\nmodel.cuda()","execution_count":null,"outputs":[]},{"metadata":{"id":"6_HVeV0eoXOI"},"cell_type":"markdown","source":"- We use AdamW optimizer which is imported earlier\n- For loss function we use Cross Entropy Loss","execution_count":null},{"metadata":{"id":"wKPtP7zaLShu","trusted":true},"cell_type":"code","source":"optimizer = AdamW(model.parameters(),lr=2e-5)# We pass model parameters","execution_count":null,"outputs":[]},{"metadata":{"id":"LXEydZNIm4-J","trusted":true},"cell_type":"code","source":"import torch.nn as nn\ncriterion = nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{"id":"nZL4HkhCLcff","trusted":true},"cell_type":"code","source":"import numpy as np\ndef flat_accuracy(preds,labels):  # A function to predict Accuracy\n  correct=0\n  for i in range(0,len(labels)):\n    if(preds[i]==labels[i]):\n      correct+=1\n  return (correct/len(labels))*100\n","execution_count":null,"outputs":[]},{"metadata":{"id":"GIaGghHQoXPN"},"cell_type":"markdown","source":"### Here our training Begins","execution_count":null},{"metadata":{"id":"Tfr3IOEoSymH","outputId":"8158327b-1eed-4679-a93b-000b3812ef77","trusted":true},"cell_type":"code","source":"no_train = 0\nepochs = 5\nfor epoch in range(epochs):\n  model.train()\n  loss1 = []\n  steps = 0\n  train_loss = []\n  l = []\n  for inputs,labels1 in loader :\n    inputs.to(device)\n    labels1.to(device)\n    optimizer.zero_grad()\n    outputs = model(inputs.to(device))\n    loss = criterion(outputs[0],labels1.to(device)).to(device)\n    logits = torch.max(outputs[0], 1)[1]\n    #ll=outp(loss)\n    [train_loss.append(p.item()) for p in torch.argmax(outputs[0],axis=1).flatten() ]#our predicted \n    [l.append(z.item()) for z in labels1]# real labels\n    loss.backward()\n    optimizer.step()\n    loss1.append(loss.item())\n    no_train += inputs.size(0)\n    steps += 1\n  print(\"Current Loss is : {} Step is : {} number of Example : {} Accuracy : {}\".format(loss.item(),epoch,no_train,flat_accuracy(train_loss,l)))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"qbM73sOCoXPl"},"cell_type":"markdown","source":"- torch.argmax() returns the index of the max number \n- axis = 1 means that it will search maximum number in a row","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\npredictions = []\n\nfor inp,lab1 in test_loader:\n  inp.to(device)\n  lab1.to(device)\n  outp1 = model(inp.to(device))\n  _, pred_label = torch.max(outp1[0], 1)\n  [predictions.append(p1.item()) for p1 in torch.argmax(outp1[0],1).flatten()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\nprint(metrics.f1_score(labels_test, predictions, average=None))\nprint(metrics.accuracy_score(labels_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}