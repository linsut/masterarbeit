{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLNet for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports for Simple Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Higher versions have problems with CUDA\n",
    "!pip install transformers==2.11.0\n",
    "!pip install simpletransformers==0.41.1\n",
    "!git clone https://github.com/NVIDIA/apex\n",
    "os.chdir('apex')\n",
    "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports for regular transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xeb1MHq-oXCA"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-transformers\n",
    "\n",
    "from transformers import XLNetTokenizer,XLNetForSequenceClassification, XLNetConfig\n",
    "from transformers import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader,RandomSampler,SequentialSampler\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files\n",
    "Only execute one of these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6s0OEaHoXDi"
   },
   "source": [
    "## Apple Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/sentiment/datasets_652925_1154930_apple-twitter-sentiment-texts.csv\")\n",
    "\n",
    "data.sentiment = data.sentiment.apply(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Airline Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/sentiment/Tweets.csv\")\n",
    "data = data[['text', 'airline_sentiment']]\n",
    "data.rename({'airline_sentiment' : 'sentiment'}, inplace=True)\n",
    "\n",
    "thisdict =\t{\n",
    "  \"negative\": 0,\n",
    "  \"neutral\": 1,\n",
    "  \"positive\": 2\n",
    "}\n",
    "data.sentiment = data.sentiment.apply(lambda x: thisdict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T4SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"data/sentiment/raw_tweets_text.csv\")\n",
    "sentiments = pd.read_csv(\".data/sentiment/t4sa_text_sentiment.csv\",delimiter = \"\\t\")\n",
    "\n",
    "tweets.set_index(tweets.id, inplace=True)\n",
    "sentiments.set_index(sentiments.TWID, inplace=True)\n",
    "data=tweets.join(sentiments)\n",
    "data.dropna(inplace=True)\n",
    "data.drop(columns=['id', 'TWID'], inplace=True)\n",
    "data[\"sentiment\"] = data[['NEU', 'NEG', 'POS']].idxmax(axis=1)\n",
    "\n",
    "data = data[['text', 'sentiment']]\n",
    "\n",
    "thisdict =\t{\n",
    "  \"NEG\": 0,\n",
    "  \"NEU\": 1,\n",
    "  \"POS\": 2\n",
    "}\n",
    "data.sentiment = data.sentiment.apply(lambda x: thisdict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text = data.text.str.lower()\n",
    "\n",
    "data.text = data.text.apply(lambda x:re.sub(r'http\\S+', '', x))\n",
    "\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "data.text = data.text.apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "data.text = data.text.apply(lambda x: ' '.join(x))\n",
    "\n",
    "data.text = data.text.map(lambda x : x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "data.text = data.text.str.replace(\"[0-9]\", \" \")\n",
    "\n",
    "data.text = data.text.str.strip(string.whitespace)\n",
    "\n",
    "df_train, df_test = train_test_split(data, test_size=0.33, random_state=42)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2BADEhO-Q5n",
    "outputId": "a9e6359b-7b5e-493b-a697-350d62367a75"
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aI3TM-V7oXEu"
   },
   "source": [
    "# Tokenization\n",
    "Only execute with non-simple Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RE440k7s-cEz"
   },
   "outputs": [],
   "source": [
    "sentences_train  = []\n",
    "for sentence in df_train['text']:\n",
    "  sentence = sentence+\"[SEP] [CLS]\"\n",
    "  sentences_train.append(sentence)\n",
    "    \n",
    "sentences_test  = []\n",
    "for sentence in df_test['text']:\n",
    "  sentence = sentence+\"[SEP] [CLS]\"\n",
    "  sentences_test.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NP1e3cNs-g3v",
    "outputId": "c51aa8b1-78c6-42a2-b36a-2853f9d5668c"
   },
   "outputs": [],
   "source": [
    "sentences_train[0] ##To check if tags are added or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfPGi_EIoXHY"
   },
   "source": [
    "### Inputs\n",
    "\n",
    "1. XLNet tokenizer is used to convert our text into tokens that correspond to   XLNetâ€™s vocabulary.\n",
    "2. a sequence of integers identifying each input token to its index number in the XLNet tokenizer \n",
    "    - Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwqJ212uAtck"
   },
   "outputs": [],
   "source": [
    "tokenizer  = XLNetTokenizer.from_pretrained('xlnet-base-cased',do_lower_case=True)\n",
    "tokenized_text_train = [tokenizer.tokenize(sent) for sent in sentences_train]\n",
    "tokenized_text_test = [tokenizer.tokenize(sent) for sent in sentences_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4O23uMZBN6K",
    "outputId": "b28dc371-defc-4581-dc7a-8522dba79b45"
   },
   "outputs": [],
   "source": [
    "tokenized_text_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ie341hRDB6T4"
   },
   "outputs": [],
   "source": [
    "ids_train = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text_train]\n",
    "ids_test = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OK-UXNt5CeTN",
    "outputId": "cfea254a-d08d-4188-cea4-d09b01055282"
   },
   "outputs": [],
   "source": [
    "\n",
    "labels_train = df_train.sentiment.values\n",
    "\n",
    "labels_test = df_test.sentiment.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONrApxqmoXJN"
   },
   "source": [
    " We find the maximum length of our sentences so that we can pad the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYx9P-pZFrU6",
    "outputId": "56c7aa88-173c-42cd-f86c-b621c6b75385"
   },
   "outputs": [],
   "source": [
    "max1 = len(ids_train[0])\n",
    "for i in ids_train:\n",
    "  if(len(i)>max1):\n",
    "    max1=len(i)\n",
    "    \n",
    "MAX_LEN_TRAIN = max1\n",
    "\n",
    "max1 = len(ids_test[0])\n",
    "for i in ids_test:\n",
    "  if(len(i)>max1):\n",
    "    max1=len(i)\n",
    "    \n",
    "MAX_LEN_TEST = max1\n",
    "\n",
    "if (MAX_LEN_TEST > MAX_LEN_TRAIN):\n",
    "    MAX_LEN = MAX_LEN_TEST \n",
    "else :\n",
    "    MAX_LEN = MAX_LEN_TRAIN\n",
    "    \n",
    "    \n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5211mavoXJt"
   },
   "source": [
    "We pad our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sz7Y0D3xGKaw"
   },
   "outputs": [],
   "source": [
    "input_ids_train2 = pad_sequences(ids_train,maxlen=MAX_LEN,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
    "input_ids_test2 = pad_sequences(ids_test,maxlen=MAX_LEN,dtype=\"long\",truncating=\"post\",padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8KPBEzPCgo9"
   },
   "outputs": [],
   "source": [
    "xtrain = input_ids_train2\n",
    "xtest = input_ids_test2\n",
    "ytrain = labels_train\n",
    "ytest = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSjgfMLxDCyi"
   },
   "outputs": [],
   "source": [
    "Xtrain = torch.tensor(xtrain)\n",
    "Ytrain = torch.tensor(ytrain)\n",
    "Xtest = torch.tensor(xtest)\n",
    "Ytest = torch.tensor(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dj1ef2DNFHLc"
   },
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrTbjxhhHsDc"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(Xtrain,Ytrain)\n",
    "test_data = TensorDataset(Xtest,Ytest)\n",
    "loader = DataLoader(train_data,batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = XLNetConfig.from_pretrained('xlnet-base-cased')\n",
    "# Set number of output labels\n",
    "config.num_labels = 3\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-ELFF0PIr81",
    "outputId": "bcb5db9c-50b1-423c-817a-fa699d3c7ce7"
   },
   "outputs": [],
   "source": [
    "model = XLNetForSequenceClassification(config)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKPtP7zaLShu"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),lr=2e-5)# We pass model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXEydZNIm4-J"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZL4HkhCLcff"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def flat_accuracy(preds,labels):  # A function to predict Accuracy\n",
    "  correct=0\n",
    "  for i in range(0,len(labels)):\n",
    "    if(preds[i]==labels[i]):\n",
    "      correct+=1\n",
    "  return (correct/len(labels))*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIaGghHQoXPN"
   },
   "source": [
    "# Training (non simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tfr3IOEoSymH",
    "outputId": "8158327b-1eed-4679-a93b-000b3812ef77"
   },
   "outputs": [],
   "source": [
    "no_train = 0\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "  loss1 = []\n",
    "  steps = 0\n",
    "  train_loss = []\n",
    "  l = []\n",
    "  for inputs,labels1 in loader :\n",
    "    inputs.to(device)\n",
    "    labels1.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs.to(device))\n",
    "    loss = criterion(outputs[0],labels1.to(device)).to(device)\n",
    "    logits = torch.max(outputs[0], 1)[1]\n",
    "    #ll=outp(loss)\n",
    "    [train_loss.append(p.item()) for p in torch.argmax(outputs[0],axis=1).flatten() ]#our predicted \n",
    "    [l.append(z.item()) for z in labels1]# real labels\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss1.append(loss.item())\n",
    "    no_train += inputs.size(0)\n",
    "    steps += 1\n",
    "  print(\"Current Loss is : {} Step is : {} number of Example : {} Accuracy : {}\".format(loss.item(),epoch,no_train,flat_accuracy(train_loss,l)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbM73sOCoXPl"
   },
   "source": [
    "- torch.argmax() returns the index of the max number \n",
    "- axis = 1 means that it will search maximum number in a row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "for inp,lab1 in test_loader:\n",
    "  inp.to(device)\n",
    "  lab1.to(device)\n",
    "  outp1 = model(inp.to(device))\n",
    "  _, pred_label = torch.max(outp1[0], 1)\n",
    "  [predictions.append(p1.item()) for p1 in torch.argmax(outp1[0],1).flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.f1_score(labels_test, predictions, average=None))\n",
    "print(metrics.accuracy_score(labels_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SimpleTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationModel('xlnet', 'xlnet-base-cased', num_labels=3, use_cuda=True, args={\n",
    "    'learning_rate':3e-5,\n",
    "    'num_train_epochs': 5,\n",
    "    'reprocess_input_data': True,\n",
    "    'overwrite_output_dir': True,\n",
    "    'process_count': 10,\n",
    "    'train_batch_size': 4,\n",
    "    'eval_batch_size': 4,\n",
    "    'max_seq_length': 512,\n",
    "    'fp16': True\n",
    "})\n",
    "\n",
    "model.train_model(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_, model_outputs_test, _ = model.eval_model(df_test)\n",
    "\n",
    "preds_test = np.argmax(model_outputs_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "print(f1_score(df_test.sentiment, preds_test, average=None))\n",
    "print(accuracy_score(df_test.sentiment, preds_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
